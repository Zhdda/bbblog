<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>CIE颜色空间一之色调、明度和饱和度</title>
      <link href="2020/12/02/CIE%E9%A2%9C%E8%89%B2%E7%A9%BA%E9%97%B4%E4%B8%80%E4%B9%8B%E8%89%B2%E8%B0%83%E3%80%81%E6%98%8E%E5%BA%A6%E5%92%8C%E9%A5%B1%E5%92%8C%E5%BA%A6/"/>
      <url>2020/12/02/CIE%E9%A2%9C%E8%89%B2%E7%A9%BA%E9%97%B4%E4%B8%80%E4%B9%8B%E8%89%B2%E8%B0%83%E3%80%81%E6%98%8E%E5%BA%A6%E5%92%8C%E9%A5%B1%E5%92%8C%E5%BA%A6/</url>
      
        <content type="html"><![CDATA[<p>在写<a href="https://www.jianshu.com/p/e5e2c013ba02">图像</a>的时候我们知道，像素构成了图片。那像素是什么？简单来说，它就是一个颜色点。那颜色怎么表示？或者说怎么度量？我们用眼睛感知到的颜色，又如何用数据表示出来呢？</p><p>接下来的几篇文章，会按照这个思路进行介绍。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-ac9c6065a11343e1.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/650" alt="img"></p><h5 id="1-颜色的几个相关术语"><a href="#1-颜色的几个相关术语" class="headerlink" title="1. 颜色的几个相关术语"></a>1. 颜色的几个相关术语</h5><p>在写图像（一）的时候，我们就知道，颜色是我们的视觉系统，对可见光的一种感知结果，感知到的颜色光波的频率决定。</p><p>而感知结果是个特别模糊的词，所以国际照明委员会（CIE，记住这个词，后面会经常出现），对这些感觉做出了定义，用颜色的三个特性，来区分颜色，那就是色调、饱和度和明度。这是颜色固有的，截然不同的三个特性。</p><p>因为制定的颜色的这三个特性，是按照主观感知结果来制定的，所以应该比较好理解。</p><h5 id="1-1-色调"><a href="#1-1-色调" class="headerlink" title="1.1 色调"></a>1.1 色调</h5><p><img src="https://upload-images.jianshu.io/upload_images/4272749-fd6ccbc8c8762b9e.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/665" alt="img"></p><p>色调表示法</p><p>查看上图，色调在颜色圆上用圆周表示，圆周上的颜色具有相同的饱和度和明度。看右图，它是最容易把颜色区分开的属性。它代表了视觉系统，对一个区域呈现的颜色的感觉。可以用红橙黄绿青蓝紫等词来描述。在术语上，把感知到的色调这种感觉，称为色彩。比如浅蓝、深蓝。黑、白、灰表示无色彩。就像我们常说的，没有色彩的人生。</p><h5 id="1-2-饱和度"><a href="#1-2-饱和度" class="headerlink" title="1.2 饱和度"></a>1.2 饱和度</h5><p><img src="https://upload-images.jianshu.io/upload_images/4272749-22bbedb395666cf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/212" alt="img"></p><p>半径表示法</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-22103ae5d66641c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/212" alt="img"></p><p>示例</p><p>如图所示，沿半径方向的颜色，具有相同的色调和明度，但他们的饱和度不同。饱和度是用来区别颜色明暗的程度，当一种颜色掺入其他光成分越多时，就说该颜色越不饱和。完全饱和的颜色，是指没有掺入白光所呈现的颜色。</p><p>在实例中所示的七种颜色，具有相同的色调和明度，但他们的饱和度不同。左边的饱和度最浅，右边的饱和度最深。</p><h5 id="1-3-明度"><a href="#1-3-明度" class="headerlink" title="1.3 明度"></a>1.3 明度</h5><p><img src="https://upload-images.jianshu.io/upload_images/4272749-47d55ffbc9434fe6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/299" alt="img"></p><p>垂直轴表示法</p><p>明度是视觉系统，对可见物体辐射光或发射光多少的感知属性。比如一支点燃的蜡烛，看起来要比在白炽灯下亮。这个词比较偏重感知，不过可以对比色调和饱和度来理解。色调其实可以理解成我们常说的不同的颜色，而饱和度可以理解成颜色的强度。</p><p>它的值用0-10表示，在右侧七种颜色中，它们具有相同的色调和饱和度，但是明度不同。底部的明度最小，顶部的明度最大。</p><p>因为明度的主观感觉值无法用物理设备测量，所以CIE后来定义了亮度（luminance），它表示单位面积上反射或发射的光的强度，这是一种可度量的表示方法，了解就行。</p><h5 id="1-4-明度和饱和度区分"><a href="#1-4-明度和饱和度区分" class="headerlink" title="1.4 明度和饱和度区分"></a>1.4 明度和饱和度区分</h5><p>评论说，关于颜色的饱和度和明度，看了文章之后还是分辨不清楚。恩，我事后深深的想了下，这点确实非常重要，因为明度和饱和度的区分，也是理解接下来颜色空间，以及颜色度量方法发展的关键。</p><p>所以接下来我准备用大白话，很明确的区分开颜色的这两个属性，在这里我们首先要明白，CIE制定的颜色的这三个属性，色调、饱和度、明度，他们是完全独立的三个属性，并且在描述一种颜色时，缺一不可。为啥？接下来就明白啦</p><h5 id="1-4-1-明度"><a href="#1-4-1-明度" class="headerlink" title="1.4.1 明度"></a>1.4.1 明度</h5><p>如果要用一句话来描述明度，那就是天黑了和天亮了~</p><p>恩，说到这你可能豁然开朗了吧，如果还不明白，请往下看</p><p>如果此刻蓝天白云太阳高照，请朝太阳的方向看，你会发现太阳旁边的天空是白色的。然后逐渐移动你的视线，远离太阳，你会发现天空越来越蓝。如果你身怀特异功能，有一双透视眼，能看到地球另一端，那么你会发现，在太阳光线触及不到的位置，也就是地平线以下，天空慢慢变灰、变暗、然后变黑。</p><p>这一过程就是只改变明度，色调和饱和度不变时，颜色的变化。</p><p>说到这我们可能明白了，明度不就是光照嘛。没有光，我们啥都看不见。</p><p>但是生活中我们可能不会太注意，光可不仅仅是影响看的见看不见这么简单。</p><p>就像上面所说的，从没有光到极强光，颜色变化为：黑-&gt;暗黑-&gt;暗-&gt;暗原色-&gt;原色-&gt;浅原色-&gt;白</p><p>一如上图：</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-47d55ffbc9434fe6.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/299" alt="img"></p><p>饱和度垂直轴表示法</p><p>所以在颜色模型里，我们通常描述明度的变化为，0到10，也就是从黑到灰到白。而且当明度为0，也就是黑色时，任何颜色都是黑色。当明度为10，也就是白色时，任何颜色都是白色。</p><p>如果你会用PS，可以新建一个图层，填充颜色为红色，然后把明度从黑移到白，来查看颜色的变化。如下图：</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-bce074a4d3478273.gif?imageMogr2/auto-orient/strip%7CimageView2/2/w/673" alt="img"></p><p>6.gif</p><h5 id="1-4-2-饱和度"><a href="#1-4-2-饱和度" class="headerlink" title="1.4.2 饱和度"></a>1.4.2 饱和度</h5><p>说完了明度，再来说饱和度应该就比较好理解了。饱和度通俗的说，可以叫纯度，或者颜色的鲜艳程度。它在半径表示法上的表现就是，从饱和度0%到饱和度为100%的过程。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-22bbedb395666cf8.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/212" alt="img"></p><p>半径表示法</p><p>而当饱和度为0时，我们看到的颜色，也就变成了明度的数值，也即黑白灰。</p><p>所以当固定明度为一定数值（为了能看见色彩，不能是0或10）时，饱和度从0%到100%变化的过程，直观的来看，也就是从：一定数值的灰度-&gt;灰原色-&gt;淡原色-&gt;纯原色</p><p>也即1.2节示例图：</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-22103ae5d66641c0.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/212" alt="img"></p><p>示例</p><p>直白的来说，就是纯红，慢慢加入一定比例数值的灰色，最终变成了灰色。</p><p>教材里说完全饱和的颜色，是指没有掺入白光所呈现的颜色。这句话也是正确的，那要怎么理解呢？</p><p>首先从颜色的定义来解释，因为颜色是我们对可见光的一种感知，而可见光属于电磁波。如果我们看见的颜色，仅由一种波长的电磁波组成，那它就是完全饱和。</p><p>如果更直白一点，就是我们慢慢的加入的一定比例的灰色，其实也是一定比例的白光。</p><h5 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h5><p>如果在PS上打开拾色器，可以看到下图</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-eae6960a2d8eaa77.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/273" alt="img"></p><p>PS拾色器</p><p>这里最左边从下到上，是明度的变化。从右到左，是饱和度的变化。可以看到在图的下方，当明度数值比较低的时候，饱和度的变化，引起的颜色变化很小。</p><h5 id="明度和饱和度区分的重要性"><a href="#明度和饱和度区分的重要性" class="headerlink" title="明度和饱和度区分的重要性"></a>明度和饱和度区分的重要性</h5><p>要明白这点，首先得明白颜色的应用场合。其实想想也明白，我们都在什么场景下会用到颜色？如果分成两大类，一个就是显示器，另一个，就是印刷品或画画等纸质操作。</p><p>显示器就不用说了，它能很好的表现出色调、饱和度、明度等属性，并且我们的显示器，普遍都能调节亮度（明度）、饱和度，甚至对比度，所以能很好的模拟现实场景。而在纸质品上，现实场景的明暗关系，通过将明度转换成从黑到白的颜料，然后在光照下观看，也能很好模拟。</p><h5 id="2-颜色空间"><a href="#2-颜色空间" class="headerlink" title="2. 颜色空间"></a>2. 颜色空间</h5><p>说完了上面这些，为啥又跳到颜色空间了？有这个问题得先弄清楚，为啥CIE要定义颜色的这几个属性，这就像我们在数学上，定义笛卡尔坐标系一样。在二维坐标或者三维空间中，我们定义坐标系，是为了准确的定义，或者找出其中某个点，就像给家家户户上门牌号一样。</p><p>而色调、饱和度、明度这几个属性呢，也是为了在颜色空间中，准确的找出是哪一种颜色。</p><p>但是有一点，颜色空间由于历史和科学的发展，产生了许许多多颜色空间，来描述颜色。</p><h5 id="2-1-颜色空间和颜色模型有啥关系？"><a href="#2-1-颜色空间和颜色模型有啥关系？" class="headerlink" title="2.1 颜色空间和颜色模型有啥关系？"></a>2.1 颜色空间和颜色模型有啥关系？</h5><p>它们互为同义词，说白了，它们是一个东西，只不过在不同的场合，叫法不一样而已。</p><p>颜色空间多数应用在数学场景下，通常用三维模型来表示，也就是代表三个参数的三维坐标来指定，它描述了颜色在颜色空间中的位置。而这三个参数，并没有直白的说明是什么颜色，这得取决于它的坐标。意思是你得拿着坐标，到颜色空间中找到它，才知道是啥颜色。</p><p>例如HSB/HSV（hue, saturation, brightness/value）颜色空间，它直接用色调、饱和度、明度作为坐标轴。色调用角度标定，红色为0度，青色为180度，饱和度用半径大小表示，明度用垂直轴表示。这种颜色空间，现在应该很熟悉了</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-84571b7a91e243ef.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/277" alt="img"></p><p>HSB/HSV颜色空间</p><h5 id="2-2-颜色空间的分类"><a href="#2-2-颜色空间的分类" class="headerlink" title="2.2 颜色空间的分类"></a>2.2 颜色空间的分类</h5><h5 id="颜色空间有设备相关和设备无关之分"><a href="#颜色空间有设备相关和设备无关之分" class="headerlink" title="颜色空间有设备相关和设备无关之分"></a>颜色空间有设备相关和设备无关之分</h5><p>因为颜色的视觉效果，是基于设备和材料的。当设备和材料改变时，对应的颜色效果就会发生变化。所以我们称这样的颜色空间，为设备相关颜色空间，如RGB和CMYK颜色空间。当不同呈色模式的设备来表现同一颜色时，对应的颜色数值是不同的。</p><p>而如在HSB的基础上，建立起来的CIE L<em>a</em>b*颜色空间（后面会讲），直接用一组数值来模拟人类的颜色视觉，而不是使用一组所需要的数值去驱动一个特定设备来生成颜色，其色彩数值与设备无关，所以这样的颜色空间称为设备无关颜色空间。</p><p>作者：金架构<br>链接：<a href="https://www.jianshu.com/p/47f0f90cb392">https://www.jianshu.com/p/47f0f90cb392</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> 音视频基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 颜色空间 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>俗称照片的彩色数字图像</title>
      <link href="2020/12/02/%E4%BF%97%E7%A7%B0%E7%85%A7%E7%89%87%E7%9A%84%E5%BD%A9%E8%89%B2%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F/"/>
      <url>2020/12/02/%E4%BF%97%E7%A7%B0%E7%85%A7%E7%89%87%E7%9A%84%E5%BD%A9%E8%89%B2%E6%95%B0%E5%AD%97%E5%9B%BE%E5%83%8F/</url>
      
        <content type="html"><![CDATA[<p>这篇文章会按照下面这个路线图进行介绍，阅读本文需要三分钟，详细阅读本文需要半小时。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-69a0a84c224a5377.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/940" alt="img"></p><p>本文路线</p><p>流程图最后一项，伽马矫正、JPEG压缩编码和文件格式，只做粗略的理论介绍。因为伽马矫正和压缩编码，需要在实际应用，并且结合其他知识来理解，所以在后面会单开几篇。</p><h5 id="1、颜色是啥和我们为什么能看到它"><a href="#1、颜色是啥和我们为什么能看到它" class="headerlink" title="1、颜色是啥和我们为什么能看到它"></a>1、颜色是啥和我们为什么能看到它</h5><p>颜色是啥，它其实是我们的视觉系统，对可见光的一种感知。如果我们是瞎子，那人类肯定不会发现还有颜色这种神奇的东西。光透过我们的眼睛，传入我们的大脑，就像照相机咔嚓一声，将一幅图片记录到内存卡。</p><p>早在之前，人类就发现<strong>光是一种电磁波</strong>，而人类并不能看见所有的电磁波，因此把能看见的电磁波，定为可见光。它的波长在380和780nm之间，</p><p>我们在自然界看到的大多数光，都不是一种波长的光，它是由许许多多，不同波长的光组合而成，因此我们才能看到这么多颜色。</p><p>就像我们看到的太阳光，它也是由多种不同颜色的光组合而成。我们小时候都玩过镜子，用一面镜子，来折射太阳光线到水里，光线再通过水折射到墙上，就出现了彩虹。而牛顿，很早的时候就用棱镜演示了这个事实：<strong>白光包含所有可见光谱的波长。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-cc07e142604178a5.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" alt="img"></p><p>光谱色</p><p>在人的视网膜中，有三种对红绿蓝敏感程度不同的椎体细胞，视网膜通过神经元感知外部世界的颜色，而每个神经元，是一个对颜色敏感的椎体。红绿蓝三种椎体细胞，对不同频率的光的感知不同，对不同亮度的感知程度也不同。所以后来人们在数字化图像的时候，面对巨大的数据量，就可以使用压缩编码技术，来降低图像的数据量，而使人眼感觉不到图像质量的下降。</p><h3 id="2、图像的颜色模型"><a href="#2、图像的颜色模型" class="headerlink" title="2、图像的颜色模型"></a>2、图像的颜色模型</h3><h5 id="2-1-RGB相加混色模型"><a href="#2-1-RGB相加混色模型" class="headerlink" title="2.1 RGB相加混色模型"></a>2.1 RGB相加混色模型</h5><p><strong>一个能发出光波的物体称为有源物体，它的颜色，由该物体发出的光波决定。</strong>就像彩色CRT一样，三个电子枪分别产生R、G、B三种波长的光，并以各种不同的相对强度组合产生不同的颜色。</p><p>因为R、G、B颜色模型是用三种光叠加，来产生特定的颜色，所以这种方法又称为RGB相加混色模型。</p><p>在自然界中，任何一种颜色，都可以用R、G、B这三种颜色值之和来确定，在数学书，它们构成一个三维的RGB矢量空间。所以只要R、G、B的数值不同，混合得到的颜色就不同，也就是光波的波长不同。</p><h5 id="3-2-第二个属性：像素深度和阿尔法（α）通道"><a href="#3-2-第二个属性：像素深度和阿尔法（α）通道" class="headerlink" title="3.2 第二个属性：像素深度和阿尔法（α）通道"></a>3.2 第二个属性：像素深度和阿尔法（α）通道</h5><p>像素深度其实就是，存储每个像素所用的位数（科普：1GB=1024MB 1MB=1024KB 1KB=1024B   B俗称字节  1字节等于八位，单位bit）。比如一个用RGB三个分量表示的彩色图像，若每个分量用8位表示，那么一个像素共用24位表示，我们就说图像的像素深度为24位</p><h5 id="3-2-1-像素深度的意义"><a href="#3-2-1-像素深度的意义" class="headerlink" title="3.2.1 像素深度的意义"></a>3.2.1 像素深度的意义</h5><p>像素深度决定了“彩色图像”的每个像素可能有的颜色数，或者是“灰度图像”的每个像素可能有的灰度级数。比如当像素深度为24位时，那么每个像素可以是16777216（2的24次方）种颜色的一种。</p><p>对于追求完美的人估计会说，如果我们提高像素深度，那每个像素能显示的颜色种类不就更多了，图像也就更细腻更自然了。</p><p>理论上确实是这样，但是一旦我们提高了像素深度，图像所对应的存储空间也急剧上升。请注意像素深度是表示一个像素所占的空间，对于一幅分辨率为640x480，像素深度为24位的图像，它所占的空间为 640 x 480 x 3 = 921600字节。</p><p>而且受到设备和人眼分辨率的限制，像素深度不一定要特别深。普通人在正常情况下，只能识别二百多种颜色，多达一千六百多万种的颜色已然足足够用。</p><h5 id="3-2-2-阿尔法（α）通道"><a href="#3-2-2-阿尔法（α）通道" class="headerlink" title="3.2.2 阿尔法（α）通道"></a>3.2.2 阿尔法（α）通道</h5><p>假设一幅图像的每个像素，用32位来表示，那么它的最高8位，就称为8位α通道，其余24位是颜色通道，红、绿、蓝分量各占一个8位通道。</p><h5 id="3-2-3-α通道的作用"><a href="#3-2-3-α通道的作用" class="headerlink" title="3.2.3 α通道的作用"></a>3.2.3 α通道的作用</h5><p>它用于表示像素在对象中的透明度（alpha）。比如用两幅图像A和B混合成一幅新图像，那么新图像C的像素为：C的像素 = （alpha）x（A的像素) + （alpha）x（B的像素) 。式中的alpha就是α的值。</p><p>再举个例子，如果一个像素用（A，R，G，B）四个分量表示，A表示α的值，取值0到1。则像素值为（1，1，0，0）时表示红色，并且红色强度为1，可以理解为完全不透明的红色。</p><p>如果像素值为（0.5，1，0，0），使用0.5乘R、G、B三个分量，得出（0.5，0.5，0，0），表示红色的强度为0.5，可以理解为红色半透明。</p><h5 id="3-3-第三个属性：真彩色、伪彩色和直接色"><a href="#3-3-第三个属性：真彩色、伪彩色和直接色" class="headerlink" title="3.3 第三个属性：真彩色、伪彩色和直接色"></a>3.3 第三个属性：真彩色、伪彩色和直接色</h5><h5 id="3-3-1-真彩色"><a href="#3-3-1-真彩色" class="headerlink" title="3.3.1 真彩色"></a>3.3.1 真彩色</h5><p>这个应该比较好理解，它是指每个像素的颜色值，用RGB表示的颜色。例如用RGB 5：5：5表示图像颜色，R、G、B各用5位，其值大小，直接确定三个基色的强度。这样得到的彩色，是真实的原图彩色。</p><h5 id="3-3-2-伪彩色"><a href="#3-3-2-伪彩色" class="headerlink" title="3.3.2 伪彩色"></a>3.3.2 伪彩色</h5><p>它指每个像素的颜色值，不是通过R、G、B三个分量直接确定的，而是通过彩色查找表，查找要显示图像的R、G、B值，以此来生成彩色。</p><p>这样解释可能不太好懂，因此我找了个例子。在有伪彩色应用的显示设备中，帧缓存显示图像的时候，会使用某一颜色值的索引，到彩色查找表里，查找对应的RGB分量值。流程图如下</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-205ca9330a2095c2.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/540" alt="img"></p><p>彩色查找表查找显示</p><p>这样通过查找得到的彩色是真的，但不是图像本身真正的颜色，它并没有反应原图真正的颜色。因此称为伪彩色</p><h5 id="3-3-3-直接色"><a href="#3-3-3-直接色" class="headerlink" title="3.3.3 直接色"></a>3.3.3 直接色</h5><p>把每个像素的RGB三个分量，作为单独的索引值对它做变换，并用变换后的R、G、B值产生颜色。注意重点是变换，至于怎么变换，可以先忽略。</p><h5 id="4-图像的种类"><a href="#4-图像的种类" class="headerlink" title="4. 图像的种类"></a>4. 图像的种类</h5><p>图像的种类常分为两种，这里说的种类，并不是指png、jpeg，这是图像的格式，而不是种类。图像的种类是指，表示图像所用的方法。</p><h5 id="4-1-位图"><a href="#4-1-位图" class="headerlink" title="4.1 位图"></a>4.1 位图</h5><p>在之前的时候，曾多次重复过栅格图像这个概念（因为是写系列文章，所以我不再重复啦）。</p><p>在描述一幅图像的时候，作为普通人，我们所认知的图像，多数是比如人物、风景等存在手机里的图片，这样的图像是使用栅格图像的方法来描述的。每幅图像，都由许多像素组成，就像我们小时候玩的拼图。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-7268d08ad7c34364.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/518" alt="img"></p><p>位图</p><p>如果作为普通用户，我们就会想，图像不都是这样嘛，不用一个像素一个像素表示，那要怎么表示呢。</p><p>举个例子，我们平常聊天的时候会发表情，而表情可以简单分为符号表情和Emoji表情（图像表情）。符号表情类似为₍₍◡( ╹◡╹ )◡₎₎  ，Emoji表情就不用说了吧。注意观察符号表情，它只是由竖线、半圆或其他几何形状表示。</p><p>而在图像的表示法里面，图像的另外一种表示方法，也类似。</p><h5 id="4-2-矢量图"><a href="#4-2-矢量图" class="headerlink" title="4.2 矢量图"></a>4.2 矢量图</h5><p>矢量图是使用点、线、弧、曲线、多边形或者其他几何形状和位置，来表示图像的一种方法。实际上就是使用各种数学表达式来描述一幅图，然后转化成计算机语音再显示出来。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-3162ce6709649aff.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/452" alt="img"></p><p>矢量图</p><p>就像描述一个三角形的时候，我们只需要知道三个点，就可以得到一幅图像。</p><h5 id="矢量图和位图对比"><a href="#矢量图和位图对比" class="headerlink" title="矢量图和位图对比"></a>矢量图和位图对比</h5><p><img src="https://upload-images.jianshu.io/upload_images/4272749-b09bf47eb2a09671.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" alt="img"></p><p>矢量图（左）和位图（右）</p><p>在矢量图表示法中，因为图中的几何形状简称对象，都是使用数学规则来描述的，所以在显示的时候，无论是放大还是缩小，或者移动旋转拷贝，都是比较容易做到的。而且在放大或缩小的时候，图像依然能够保持清晰。</p><p>为什么？就拿矩形来说，如果我放大了，矩形的四个点，只是在屏幕上的坐标位置变了下，中间依然是靠线段连接，如左图，图像并不会失真。它并不会像位图那样，放大到最后，会看到图像的像素块，如右图。</p><p>而且矢量图在计算机存储的时候，存储的是数学规则，并且可以把类似的图形做统一处理，所以在文件大小方面较好掌控。而位图则是把每个像素都存起来，图像分辨率高、像素深度深的时候，占的空间会比较大。</p><h5 id="那为什么我们手机里的图像，还是位图呢？"><a href="#那为什么我们手机里的图像，还是位图呢？" class="headerlink" title="那为什么我们手机里的图像，还是位图呢？"></a>那为什么我们手机里的图像，还是位图呢？</h5><p>第一点，这是因为，真实世界里的景象纷杂无序，很难用规则的几何图形去表达。</p><p>第二点，是因为，真实世界里色彩纷杂，颜色变化阴暗透明起承转合，如果几何图形再加上颜色变化，难上加难。</p><p>所以总结起来，位图较多用来描述风景、人物，等不规则且色彩丰富的图像。而矢量图因为它无限放大而不失真的特性，在一些文字设计、标志设计方面应用广泛。</p><h5 id="5-灰度图和彩色图像"><a href="#5-灰度图和彩色图像" class="headerlink" title="5. 灰度图和彩色图像"></a>5. 灰度图和彩色图像</h5><h5 id="5-1-灰度图"><a href="#5-1-灰度图" class="headerlink" title="5.1 灰度图"></a>5.1 灰度图</h5><p>这两种比较好理解，灰度图就是只有明暗像素，没有彩色像素组成的图像。就像我们小时候看的黑白电视。</p><p>再细分下去，当只有黑白两种颜色时的图像，称为单色图像。这样一来，它的每个像素的像素值，用一位（一个比特位/bit）来存储，值为”0“或”1“，像素点非黑即白。</p><p>而一个标准的灰度图，一个像素用一个字节（8位）表示，这样每个像素的值就可以为，0~255（二的八次方）之间的任意值，称为256级灰度。当图像的分辨率为640X480的时候，它的存储空间为300KB（640*480/1024）。</p><p>下图是<strong>标准单色图</strong>和<strong>标准灰度图</strong>的对比。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-a03ed33d57e79c3a.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/986" alt="img"></p><p>标准单色图和标准灰度图的对比</p><h5 id="5-2-彩色图像"><a href="#5-2-彩色图像" class="headerlink" title="5.2 彩色图像"></a>5.2 彩色图像</h5><p>现在看到这个词应该很熟悉吧，说一下它的特性会更熟悉。彩色图像的每个像素，包含了图像的颜色信息。</p><p>在划分彩色图像的时候，一般用颜色的数目来划分。比如：</p><p>256色图像：它是指像素的R、G、B三个值，用一个字节来表示，这样一个像素点可能的颜色值，就为256（二的八次方）色之一。</p><p>真彩色图像：每个像素的R、G、B值，分别用一个字节来表示。这样一个像素点可能的颜色值，就为1677万（二的二十四次方）多色之一。这里的真彩色图像，相对于<a href="https://www.jianshu.com/p/e5e2c013ba02">图像（一）</a>中，又多了一层含义。它表示了这种图像，达到了人眼分辨率的极限，反应了原图的真实色彩。故称为真彩色。当图像的分辨率为640X480时，它的存储空间为900KB（640<em>480</em>3/1024）。</p><h5 id="6-伽马矫正"><a href="#6-伽马矫正" class="headerlink" title="6. 伽马矫正"></a>6. 伽马矫正</h5><p>这个词应该很多人都听过，即使不是专业人士，也可能早有耳闻。</p><p>在很多教材中，说之所以有伽马矫正这个东西，是因为摄像机或手机的摄像头，在采集自然界图像的时候，光的摄入和摄像机电压的转换，并不是线性的。</p><p>而在显示的时候，CRT发射的光的强度，和它的输入电压也不是线性的。</p><p>这样就会导致，从采集的一幅图像，到存储成图像文件，再到读出图像文件，在显示器上显示的时候，我们看到的显示出来的图像，并不是最开始采集的图像。</p><p>为了解决这个问题，就有了伽马矫正这个东西。</p><p>它在采集和显示图像的时候，分别进行矫正，以达到还原原始场景的目的。</p><p>在伽马矫正这个问题上，讨论很多，知乎上有一个高票答案，讨论关于广义的伽马矫正，有兴趣的朋友可以看下，真的很有趣，链接如下：</p><p><a href="https://link.jianshu.com/?t=https://www.zhihu.com/question/27467127">https://www.zhihu.com/question/27467127</a></p><h5 id="7-图像的压缩编码"><a href="#7-图像的压缩编码" class="headerlink" title="7. 图像的压缩编码"></a>7. 图像的压缩编码</h5><p>具体的压缩编码后面再讲，这里只粗略讲一下，为啥要进行压缩编码。</p><p>恩，如果不进行压缩编码，图像会很大，完全不利于网络传输。而且普通人只能分辨二百多种颜色，多了也看不出来，所以1677万完全浪费。</p><p>所以后来就有了JPEG压缩编码，注意这里说的JPEG，是指一种图像数据的压缩编码标准，并不是JPEG格式。</p><h5 id="什么是格式，什么是标准"><a href="#什么是格式，什么是标准" class="headerlink" title="什么是格式，什么是标准"></a>什么是格式，什么是标准</h5><p>标准是指，在压缩图像数据的时候，该采用什么算法、怎样的流程去压缩，注意目的是怎么压缩。</p><p>而格式是指，存放使用JPEG标准压缩的图像文件格式。</p><h5 id="8-图像的文件格式"><a href="#8-图像的文件格式" class="headerlink" title="8. 图像的文件格式"></a>8. 图像的文件格式</h5><p>自从图像进入计算机以来，已经开发出了很多种图像文件存储格式，而且互不兼容。有些不兼容的格式，使用起来很不方便，需要使用特定的软件，因此有些格式也在逐渐被淘汰。</p><p>目前流行的图像格式为：GIF、JPEG、PNG。</p><p>具体他们之间有什么区别，又是如何产生，有何渊源，等用到的时候再写，用不到就自动忽略了~~~</p><p>作者：金架构<br>链接：<a href="https://www.jianshu.com/p/0529ced280e0">https://www.jianshu.com/p/0529ced280e0</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> 音视频基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 数字图像 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>显示器是如何显示图形数据的</title>
      <link href="2020/12/02/%E6%98%BE%E7%A4%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E5%BD%A2%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80/"/>
      <url>2020/12/02/%E6%98%BE%E7%A4%BA%E5%99%A8%E6%98%AF%E5%A6%82%E4%BD%95%E6%98%BE%E7%A4%BA%E5%9B%BE%E5%BD%A2%E6%95%B0%E6%8D%AE%E7%9A%84%E4%B8%80/</url>
      
        <content type="html"><![CDATA[<h3 id="1-最早的显示器"><a href="#1-最早的显示器" class="headerlink" title="1. 最早的显示器"></a>1. 最早的显示器</h3><p>在十八世纪，人类对电的研究渐渐成熟，而电在图形学方面的应用却还是一个空白，除了在世纪初的时候，电弧光灯率先被发明，它打开了人类对光的探索。人类开始猜想，我们是不是可以把一个活动的景象，通过电来传送并且复现。</p><p>开始的时候，<strong>行扫描</strong>概念被提出，也就是可以把一幅图像，分成一行一行进行扫描，传真装置因此诞生。之后，有一个人提出，我们是不是可以把图像分为栅格的形式，也就一个一个小格子，每个格子显示一个颜色，拼接起来，就是完整的一幅图像，这个学说被沿用至今。而光有<strong>行扫描</strong>和<strong>栅格图像</strong>还不够，因为虽然学说有了，但是还是没有把图片数字化。而在同一时间，光电管被发明，这种装备可以实现光和电的互相转换。另外一个哥们在这些基础上，结合电磁波载波，发明了电视，而那时候的显示，只能达到每秒5帧，每帧（关于帧，下面会进行说明）30行。</p><h3 id="2-CRT显示器"><a href="#2-CRT显示器" class="headerlink" title="2. CRT显示器"></a>2. CRT显示器</h3><p>在十八世纪末的时候，有一个更牛逼的哥们发明了一件装备，它能把电信号转化为光信号。听着像上面提到的光电管，然而并不是，它被称为<strong>阴极射线管</strong>，英文名<strong>CRT（Cathode Ray Tubes）</strong>。此后诞生了一系列电子显示器（CRT显示器），在开始详细介绍它之前，先来看一组图片。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-96f8b8770055fd4f.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/490" alt="img"></p><p>示例图0</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-9be82dee2f0ca554.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/415" alt="img"></p><p>示例图1</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-9088b82e6f5fe773.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/400" alt="img"></p><p>示例图2</p><p>别看这些大块头，现在觉得很Low又笨重。事实上，在高中的时候我们机房用的都是这种显示器。它奠定和引导了显示器的突飞猛进，它同时也证明了一个定理，一个新事物只有起步的时候是最难的，你若坚持，此后一帆风顺。</p><p>好了，现在来看一下CRT的原理图，它是了解和处理视频数据的基础。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-2696c58f838fdf51.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/774" alt="img"></p><p>CRT的原理图</p><p>这个就是CRT，看着像手电筒，其实并不是。手电筒产生的是散光或不均匀光，而CRT产生的是均匀光。它的原理是通过电子枪发射“一束”电子，通过聚焦、偏转等手段，打到涂有荧光层的屏幕上进行发光。在前面的栅格图像学说的理论之下，由一束电子点亮的屏幕一点，称为一个<strong>像素</strong>。而CRT无重叠的打到屏幕上的最多点数称为<strong>分辨率</strong>，如我们熟知的1280X1024，它表示屏幕上最多有1280X1024个点可以被点亮。</p><p><strong>好的显示器，在同样的物理尺寸的情况下，像素点更小，能够显示的点数更多，分辨率更大，因而画质更细腻。</strong></p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-4829fdd513e30c25.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1200" alt="img"></p><p>CRT显示器的扫描原理</p><p>这张图是CRT显示器的扫描原理，也是著名的<strong>光栅扫描显示器</strong>（光栅一词来源于光学，它的本意是，由大量等宽等间距的平行狭缝构成的光学器件。而CRT显示器一行行扫描，形成的图案，类似光栅）。典型的CRT采取<strong>逐行扫描</strong>的方式，当然也有<strong>隔行扫描</strong>和<strong>随机扫描</strong>的方式，不过在本文中不做研究。</p><p>在电子束沿每一行扫描的时候，它的强度要不断变化，从而生成一幅图像。这个图像的图形定义，也就是每一个像素的<strong>亮度、颜色</strong>等信息，保存在<strong>帧缓存（刷新缓存/颜色缓存）</strong>里面，<strong>帧是指整个屏幕</strong>。电子束从首行开始，到扫完尾行完成一幅图案的显示，这称为<strong>一帧</strong>。CRT每完成一帧，自动返回到左上角，开始扫描下一帧。</p><p>而<strong>人眼在每秒25帧的情况下，能自动把图片视为连续的动画，30帧以上，会觉得不卡。</strong>现在普遍的显示器，每秒都能打到60帧以上，大多数能达到100多帧。例如随便搜个显示器，如下图</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-c94731dc3c78bfcb.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/776" alt="img"></p><p>某显示器参数</p><p>它的响应时间（点亮一像素所需要的时间）为5ms，每秒最多能显示1000ms/5ms=200帧</p><p>在显示器（一）中，展示了单个电子枪，发射电子到荧光屏幕，会点亮一个像素。而且不难猜到，这个像素的颜色，只能是黑或白，或者它们的混合色，也就是灰色，就像我们小时候看的黑白电视。</p><h3 id="3-彩色CRT"><a href="#3-彩色CRT" class="headerlink" title="3. 彩色CRT"></a>3. 彩色CRT</h3><p>而如果要点亮一个<strong>彩色像素点</strong>，就需要<strong>彩色CRT</strong>，如下图所示。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-39c9b98d0eb512e3.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/660" alt="img"></p><p>彩色CRT</p><p>彩色CRT的原理，就是分别点亮R（红）、G（绿）、B（蓝）三基色的荧光点，每个荧光点发出对应强度值的光，因为三个荧光点距离非常小，从而在人眼看来，看到的是三个荧光点组合之后的彩色亮光。</p><p>在CRT之后，又出现了一大批显示器，如等离子显示器、LED显示器、LCD显示器，虽然成像方式变了，但是成像原理并没有变。所以对于做音视频开发的从业人员，它们之间的区别可以忽视。</p><h3 id="4-光栅扫描系统"><a href="#4-光栅扫描系统" class="headerlink" title="4. 光栅扫描系统"></a>4. 光栅扫描系统</h3><p>虽然成像方式之间的区别可以忽视，但对于开发者来说，图像的显示流程是绝不可以忽视的。</p><p>伴随着CRT的产生，人们根据显示器的显示原理，定义出了一系列的计算机图形学名词，例如帧缓存（在今天，可以简单的理解为显存）、帧率（每秒显示帧数）、分辨率，以方便描述显示器是如何工作的。</p><p>而且我们知道，一幅图像可以通过行扫描、和光栅图像，被数字化并保存到帧缓存里。然后电子枪根据帧缓存里，保存的每一像素的强度、颜色等信息，调节自身电压、并在荧光屏幕上打出对应的像素点。</p><p>如果把这一流程进行模块化，其实只有简单的两个模块，那就是<strong>帧缓存</strong>和<strong>CRT（显示器）</strong>。</p><p>在后来研发的图形用户界面的计算机系统中，这两个模块，被集成到了计算机中，他们通过视频控制器来支配。下图为最简单的图形用户界面计算机的显示流程，因为这一流程，是在显示器显示原理之下发展出来的，所以通常称为<strong>光栅扫描系统</strong>。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-2a0f17fe0fbfc714.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1028" alt="img"></p><p>较早的光栅扫描系统</p><p>右侧的监视器就是我们的显示器，而帧缓存和其他数据一样，被放在了系统存储器中。视频控制器连接总线，提取像素信息，进行图形数据的显示。后来，人们又在这一基础上进行优化。于是变成了这样子</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-7cdab68b66b73ca4.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1194" alt="img"></p><p>改进的光栅扫描系统</p><p>系统存储器被开辟出一个固定区域，用来存储帧缓存，由视频控制器直接访问。</p><p>这样一来，它的显示流程一步步清晰和模块化。</p><p>在显示图像的时候，视频控制器将两个寄存器（存放变量的地方）x和y分别置为0和y_max，搞两个寄存器是因为显示器是一个二维笛卡尔坐标系，因此屏幕上任一点都可以根据x和y值来锁定。它的原点在左下角，向右和向上分别为x和y正方向。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-4e4bd2e47b79e5b2.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/427" alt="img"></p><p>而光栅扫描是从左上角开始，所以开始时的坐标为（0，y_max）。</p><p>然后CPU根据坐标位置，到存储器取出帧缓存中，对应的像素值，显示器根据像素值进行显示。</p><p>后来由于计算机图形学的迅猛发展，需要处理的图形信息越来越复杂，单靠CPU处理图形数据，已经不能满足需求。于是诞生了<strong>显示处理器（GPU）</strong>，俗称<strong>显卡</strong>。而帧缓存，也从系统存储器中，挪到了显示处理器存储器（<strong>显存</strong>）中来存储。</p><p><img src="https://upload-images.jianshu.io/upload_images/4272749-72b792be02dd2a5f.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/690" alt="img"></p><p>最新的光栅扫描系统</p><p>至此，关于显示器这方面，图形数据的显示先告一段落。接下来会从图像（也就是我们平常拍的照片）入手，来讨论<strong>图像的数字化过程</strong>，期间也会进一步加深对光栅扫描系统的理解~</p><h3 id="关于学习资源"><a href="#关于学习资源" class="headerlink" title="关于学习资源"></a>关于学习资源</h3><p>如果想更详细的了解这一部分内容，可以查阅<strong>《计算机图形学》</strong>，以下为PDF版：<br> <a href="https://pan.baidu.com/s/1c1WPxVq">https://pan.baidu.com/s/1c1WPxVq</a></p><p>作者：金架构<br>链接：<a href="https://www.jianshu.com/p/7392dc5a4e05">https://www.jianshu.com/p/7392dc5a4e05</a><br>来源：简书<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。</p>]]></content>
      
      
      <categories>
          
          <category> 音视频基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> -显示器 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>H264基础知识</title>
      <link href="2020/12/02/H264%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/"/>
      <url>2020/12/02/H264%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86/</url>
      
        <content type="html"><![CDATA[<blockquote><p>版权声明：本文为博主原创文章，遵循<a href="http://creativecommons.org/licenses/by-sa/4.0/"> CC 4.0 BY-SA </a>版权协议，转载请附上原文出处链接和本声明。</p><p>本文链接：<a href="https://blog.csdn.net/go_str/article/details/80340564">https://blog.csdn.net/go_str/article/details/80340564</a></p></blockquote><h2 id="H264概述"><a href="#H264概述" class="headerlink" title="H264概述"></a>H264概述</h2><p>H264 是 MPEG-4 标准所定义的编码格式，标准写法应该是H.264。</p><p>H264 视频格式是经过<strong>有损压缩</strong>的，但在技术上尽可能做的降低存储体积下获得较好图像质量和低带宽图像快速传输。 </p><p>H264压缩技术主要采用了以下几种方法对视频数据进行压缩。包括：</p><ul><li>帧内预测压缩，解决的是空域数据冗余问题。</li><li>帧间预测压缩（运动估计与补偿），解决的是时域数据冗余问题。</li><li>整数离散余弦变换（DCT），将空间上的相关性变为频域上无关的数据然后进行量化。</li><li>CABAC压缩。</li></ul><p><strong>H264结构中，一个视频图像编码后的数据叫做一帧，一帧由一个片（slice）或多个片组成，一个片由一个或多个宏块（MB）组成，一个宏块由16x16的yuv数据组成。宏块作为H264编码的基本单位。</strong></p><p>在H264协议内定义了三种帧，分别是I帧、B帧与P帧。I帧就是之前所说的一个完整的图像帧，而B、帧与P帧所对应的就是之前说的不编码全部图像的帧。P帧与B帧的差别就是P帧是参考之前的I帧而生成的，而B帧是参考前后图像帧编码生成的。</p><p>经过压缩后的帧分为：<strong>I帧，P帧和B帧</strong>:</p><ul><li>I帧：关键帧，采用<strong>帧内压缩</strong>技术。你可以理解为这一帧画面的完整保留；解码时只需要本帧数据就可以完成（因为包含完整画面）</li><li>P帧：向前参考帧，在压缩时，只参考前面已经处理的帧。采用<strong>帧间压缩</strong>技术。P帧表示的是这一帧跟之前的一个关键帧（或P帧）的差别，解码时需要用之前缓存的画面叠加上本帧定义的差别，生成最终画面。（也就是差别帧，P帧没有完整画面数据，只有与前一帧的画面差别的数据）</li><li>B帧：双向参考帧，在压缩时，它既参考前而的帧，又参考它后面的帧。采用<strong>帧间压缩</strong>技术。B帧记录的是本帧与前后帧的差别（具体比较复杂，有4种情况），换言之，要解码B帧，不仅要取得之前的缓存画面，还要解码之后的画面，通过前后画面的与本帧数据的叠加取得最终的画面。B帧压缩率高，但是解码时CPU会比较累~。</li></ul><p>还有一个概念是，<strong>IDR帧</strong>：</p><p>一个序列的第一个图像叫做 IDR 图像（立即刷新图像），<strong>IDR 图像都是 I 帧图像</strong>。<strong>H.264 引入 IDR 图像是为了解码的重同步，当解码器解码到 IDR 图像时，立即将参考帧队列清空，将已解码的数据全部输出或抛弃，重新查找参数集，开始一个新的序列</strong>。这样，如果前一个序列出现重大错误，在这里可以获得重新同步的机会。<strong>IDR图像之后的图像永远不会使用IDR之前的图像的数据来解码</strong>。<strong>IDR 图像一定是 I 图像，但I图像不一定是 IDR 图像</strong>。一个序列中可以有很多的I图像，I 图像之后的图像可以引用 I 图像之间的图像做运动参考。</p><p>还有一点注意的，对于 IDR 帧来说，在 <strong>IDR 帧之后的所有帧都不能引用任何 IDR 帧之前的帧</strong>的内容，与此相反，对于普通的 I 帧来说，位于其之后的 B- 和 P- 帧可以引用位于普通 I- 帧之前的 I- 帧。从随机存取的视频流中，播放器永远可以从一个 IDR 帧播放，因为在它之后没有任何帧引用之前的帧。但是，不能在一个没有 IDR 帧的视频中从任意点开始播放，因为后面的帧总是会引用前面的帧。</p><p>继续再多补充一个概念，<strong>图像组（**</strong>GOP）**：</p><p>一个序列就是一段内容差异不太大的图像编码后生成的一串数据流。当运动变化比较少时，一个序列可以很长，因为运动变化少就代表图像画面的内容变动很小，所以就可以编一个 I 帧，然后一直 P 帧、B 帧了。当运动变化多时，可能一个序列就比较短了，比如就包含一个 I 帧和 3、4个P帧。</p><p>GOP是画面组，一个GOP是一组连续的画面。<br>GOP一般有两个数字，如M=3，N=12。M指定I帧与P帧之间的距离，N指定两个I帧之间的距离。那么现在的GOP结构是：</p><p><img src="https://img-blog.csdnimg.cn/20200519205304320.png" alt="img"></p><p>I 帧、B帧、P帧还有一些特点，如下：<br><strong>I帧特点:</strong><br>1)它是一个全帧压缩编码帧。它将全帧图像信息进行JPEG压缩编码及传输;<br>2)解码时仅用I帧的数据就可重构完整图像;<br>3)I帧描述了图像背景和运动主体的详情;<br>4)I帧不需要参考其他画面而生成;<br>5)I帧是P帧和B帧的参考帧(其质量直接影响到同组中以后各帧的质量);<br>6)I帧是帧组GOP的基础帧(第一帧),在一组中只有一个I帧;<br>7)I帧不需要考虑运动矢量;<br>8)I帧所占数据的信息量比较大。</p><p><strong>P帧特点:</strong><br>1)P帧是I帧后面相隔1~2帧的编码帧;<br>2)P帧采用运动补偿的方法传送它与前面的I或P帧的差值及运动矢量(预测误差);<br>3)解码时必须将I帧中的预测值与预测误差求和后才能重构完整的P帧图像;<br>4)P帧属于前向预测的帧间编码。它只参考前面最靠近它的I帧或P帧;<br>5)P帧可以是其后面P帧的参考帧,也可以是其前后的B帧的参考帧;<br>6)由于P帧是参考帧,它可能造成解码错误的扩散;<br>7)由于是差值传送,P帧的压缩比较高。</p><p><strong>B帧特点：</strong><br>1）B帧是由前面的I或P帧和后面的P帧来进行预测的;<br>2）B帧传送的是它与前面的I或P帧和后面的P帧之间的预测误差及运动矢量;<br>3）B帧是双向预测编码帧;<br>4）B帧压缩比最高,因为它只反映并参考帧间运动主体的变化情况,预测比较准确;加大B帧的数量可以有效地提高视频数据的压缩比，但是在实时互动的环境下，过多的B帧会引起延时，因为B帧会过分的依赖于前后帧，在网络好的环境下，可以正常的传输帧，这样没有什么问题，但是在网络不好的时候，B帧会等待其他帧到来，会引起延时。<br>5）B帧不是参考帧,不会造成解码错误的扩散。</p><p>注:I、B、P各帧是根据压缩算法的需要，是人为定义的,它们都是实实在在的物理帧。一般来说，I帧的压缩率是7（跟JPG差不多），P帧是20，B帧可以达到50。可见使用B帧能节省大量空间，节省出来的空间可以用来保存多一些I帧，这样在相同码率下，可以提供更好的画质。</p><p>备注：</p><p>视频传输中会出现连个比较常见的现象，花屏 和 卡顿</p><p>(1)如果在GOP分组中的P帧丢失，会造成解码端的图像发生错误。这就是花屏。GOP一组帧呈现出的连贯效果，由于P帧丢失，它需要更新的部分就没有，所以无法正常呈现。故出现花屏现象。</p><p>(2)为了解决花屏的问题发生，我们可以将丢失 P帧 或是 I帧 的 GOP 丢掉（包含其中的所有帧），直到下一个I帧再重新刷新图像。但是由于这一帧丢掉了，所以会出现卡顿。</p><h2 id="H264压缩技术"><a href="#H264压缩技术" class="headerlink" title="H264压缩技术"></a>H264压缩技术</h2><p>H264的基本原理其实非常简单，我们就简单的描述一下H264压缩数据的过程。通过摄像头采集到的视频帧（按每秒 30 帧算），被送到 H264 编码器的缓冲区中。编码器先要为每一幅图片划分宏块。</p><p>H264采用的核心算法是<strong>帧内压缩和帧间压缩</strong>，帧内压缩是生成I帧的算法，帧间压缩是生成B帧和P帧的算法。</p><p><strong>帧内（**</strong>Intraframe）压缩也称为空间压缩（Spatialcompression）<strong>。当压缩一帧图像时，仅考虑本帧的数据而不考虑相邻帧之间的冗余信息，这实际上与静态图像压缩类似。</strong>帧内一般采用有损压缩算法**，由于帧内压缩是编码一个完整的图像，所以可以独立的解码、显示。帧内压缩一般达不到很高的压缩，跟编码jpeg差不多。</p><p><strong>帧间（**</strong>Interframe）压缩<strong>的原理是：相邻几帧的数据有很大的相关性，或者说前后两帧信息变化很小的特点。也即连续的视频其相邻帧之间具有冗余信息,根据这一特性，压缩相邻帧之间的冗余量就可以进一步提高压缩量，减小压缩比。帧间压缩也称为时间压缩（Temporalcompression），它通过比较时间轴上不同帧之间的数据进行压缩。</strong>帧间压缩一般是无损的**。帧差值（Framedifferencing）算法是一种典型的时间压缩法，它通过比较本帧与相邻帧之间的差异，仅记录本帧与其相邻帧的差值，这样可以大大减少数据量。</p><h3 id="压缩方式说明"><a href="#压缩方式说明" class="headerlink" title="压缩方式说明"></a>压缩方式说明</h3><p>Step1：分组，也就是将一系列变换不大的图像归为一个组，也就是一个序列，也可以叫GOP（画面组）；</p><p>Step2：定义帧，将每组的图像帧归分为I帧、P帧和B帧三种类型；</p><p>Step3：预测帧， 以I帧做为基础帧,以I帧预测P帧,再由I帧和P帧预测B帧;</p><p>Step4：数据传输， 最后将I帧数据与预测的差值信息进行存储和传输。</p><h4 id="划分宏块"><a href="#划分宏块" class="headerlink" title="划分宏块"></a>划分宏块</h4><p>H264默认是使用 16X16 大小的区域作为一个宏块，也可以划分成 8X8 大小。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412771.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>划分好宏块后，计算宏块的象素值。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412770.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>以此类推，计算一幅图像中每个宏块的像素值，所有宏块都处理完后如下面的样子。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412807.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><ol><li><ol><li><strong>划分子块</strong></li></ol></li></ol><p>H264对比较平坦的图像使用 16X16 大小的宏块。但为了更高的压缩率，还可以在 16X16 的宏块上更划分出更小的子块。子块的大小可以是 8X16､ 16X8､ 8X8､ 4X8､ 8X4､ 4X4非常的灵活。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412769.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>上幅图中，红框内的 16X16 宏块中大部分是蓝色背景，而三只鹰的部分图像被划在了该宏块内，为了更好的处理三只鹰的部分图像，H264就在 16X16 的宏块内又划分出了多个子块。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412807.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>这样再经过帧内压缩，可以得到更高效的数据。下图是分别使用mpeg-2和H264对上面宏块进行压缩后的结果。其中左半部分为MPEG-2子块划分后压缩的结果，右半部分为H264的子块划压缩后的结果，可以看出H264的划分方法更具优势。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412890.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>宏块划分好后，就可以对H264编码器缓存中的所有图片进行分组了。</p><h4 id="帧分组"><a href="#帧分组" class="headerlink" title="帧分组"></a>帧分组</h4><p>对于视频数据主要有两类数据冗余，一类是时间上的数据冗余，另一类是空间上的数据冗余。其中时间上的数据冗余是最大的。下面我们就先来说说视频数据时间上的冗余问题。</p><p>为什么说时间上的冗余是最大的呢？假设摄像头每秒抓取30帧，这30帧的数据大部分情况下都是相关联的。也有可能不止30帧的的数据，可能几十帧，上百帧的数据都是关联特别密切的。</p><p>对于这些关联特别密切的帧，其实我们只需要保存一帧的数据，其它帧都可以通过这一帧再按某种规则预测出来，所以说视频数据在时间上的冗余是最多的。</p><p>为了达到相关帧通过预测的方法来压缩数据，就需要将视频帧进行分组。那么如何判定某些帧关系密切，可以划为一组呢？我们来看一下例子，下面是捕获的一组运动的台球的视频帧，台球从右上角滚到了左下角。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412855.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"><img src="https://img-blog.csdnimg.cn/20200519205412853.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>H264编码器会按顺序，每次取出两幅相邻的帧进行宏块比较，计算两帧的相似度。如下图：</p><p><img src="https://img-blog.csdnimg.cn/20200519205412856.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>通过宏块扫描与宏块搜索可以发现这两个帧的关联度是非常高的。进而发现这一组帧的关联度都是非常高的。因此，上面这几帧就可以划分为一组。<strong>其算法是：在相邻几幅图像画面中，一般有差别的像素只有*<strong>*<em>\</em>10%以内的点,亮度差值变化不超过2%，而色度差值的变化只有1%以内，我们认为这样的图可以分到一组。\</strong></strong></p><p>在这样一组帧中，经过编码后，我们只保留第一帧的完整数据，其它帧都通过参考上一帧计算出来。我们称第一帧为*<strong>*IDR／I帧**</strong>，其它帧我们称为*<strong>*P／B帧**</strong>，这样编码后的数据帧组我们称为*<strong>*GOP**</strong>。</p><h4 id="运动估计与补偿"><a href="#运动估计与补偿" class="headerlink" title="运动估计与补偿"></a>运动估计与补偿</h4><p>在H264编码器中将帧分组后，就要计算帧组内物体的运动矢量了。还以上面运动的台球视频帧为例，我们来看一下它是如何计算运动矢量的。</p><p>H264编码器首先按顺序从缓冲区头部取出两帧视频数据，然后进行宏块扫描。当发现其中一幅图片中有物体时，就在另一幅图的邻近位置（搜索窗口中）进行搜索。如果此时在另一幅图中找到该物体，那么就可以计算出物体的运动矢量了。下面这幅图就是搜索后的台球移动的位置。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412855.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>通过上图中台球位置相差，就可以计算出台球运行的方向和距离。H264依次把每一帧中球移动的距离和方向都记录下来就成了下面的样子。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412856.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>运动矢量计算出来后，将相同部分（也就是绿色部分）减去，就得到了补偿数据。我们最终只需要将补偿数据进行压缩保存，以后在解码时就可以恢复原图了。压缩补偿后的数据只需要记录很少的一点数据。如下所示：</p><p><img src="https://img-blog.csdnimg.cn/20200519205412858.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>我们把运动矢量与补偿称为<strong>帧间压缩技术</strong>，它解决的是视频帧在时间上的数据冗余。除了帧间压缩，帧内也要进行数据压缩，帧内数据压缩解决的是空间上的数据冗余。下面我们就来介绍一下帧内压缩技术。</p><h4 id="帧内预测"><a href="#帧内预测" class="headerlink" title="帧内预测"></a>帧内预测</h4><p>人眼对图象都有一个识别度，对低频的亮度很敏感，对高频的亮度不太敏感。所以基于一些研究，可以将一幅图像中人眼不敏感的数据去除掉。这样就提出了帧内预测技术。</p><p>H264的帧内压缩与JPEG很相似。一幅图像被划分好宏块后，对每个宏块可以进行 9 种模式的预测。找出与原图最接近的一种预测模式。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412887.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>下面这幅图是对整幅图中的每个宏块进行预测的过程。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412889.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>帧内预测后的图像与原始图像的对比如下：</p><p><img src="https://img-blog.csdnimg.cn/20200519205412888.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>然后，将原始图像与帧内预测后的图像相减得残差值。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412858.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>再将我们之前得到的预测模式信息一起保存起来，这样我们就可以在解码时恢复原图了。效果如下：</p><p><img src="https://img-blog.csdnimg.cn/20200519205412884.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>经过帧内与帧间的压缩后，虽然数据有大幅减少，但还有优化的空间。</p><h4 id="对残差数据做DCT"><a href="#对残差数据做DCT" class="headerlink" title="对残差数据做DCT"></a>对残差数据做DCT</h4><p>可以将残差数据做整数离散余弦变换，去掉数据的相关性，进一步压缩数据。如下图所示，左侧为原数据的宏块，右侧为计算出的残差数据的宏块。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412857.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>将残差数据宏块数字化后如下图所示：</p><p><img src="https://img-blog.csdnimg.cn/20200519205412884.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>将残差数据宏块进行 DCT 转换。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412887.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>去掉相关联的数据后，我们可以看出数据被进一步压缩了。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412889.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>做完 DCT 后，还不够，还要进行 CABAC 进行无损压缩。</p><h4 id="CABAC"><a href="#CABAC" class="headerlink" title="CABAC"></a>CABAC</h4><p>上面的帧内压缩是属于有损压缩技术。也就是说图像被压缩后，无法完全复原。而CABAC属于无损压缩技术。</p><p>无损压缩技术大家最熟悉的可能就是哈夫曼编码了，给高频的词一个短码，给低频词一个长码从而达到数据压缩的目的。MPEG-2中使用的VLC就是这种算法，我们以 A-Z 作为例子，A属于高频数据，Z属于低频数据。看看它是如何做的。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412891.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>CABAC也是给高频数据短码，给低频数据长码。同时还会根据上下文相关性进行压缩，这种方式又比VLC高效很多。其效果如下：</p><p><img src="https://img-blog.csdnimg.cn/20200519205412889.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>现在将 A-Z 换成视频帧，它就成了下面的样子。</p><p><img src="https://img-blog.csdnimg.cn/20200519205412890.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>从上面这张图中明显可以看出采用 CACBA 的无损压缩方案要比 VLC 高效的多。</p><h1 id="H264分层结构"><a href="#H264分层结构" class="headerlink" title="H264分层结构"></a>H264分层结构</h1><p>H264的主要目标是为了有高的视频压缩比和良好的网络亲和性，为了达成这两个目标，H264的解决方案是将系统框架分为两个层面，分别是视频编码层面（VCL）和网络抽象层面（NAL），如图；</p><p><img src="https://img-blog.csdnimg.cn/20200519205514394.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>H.264原始码流(裸流)是由一个接一个NALU组成，它的功能分为两层，VCL(视频编码层)和 NAL(网络抽象层).</p><table><thead><tr><th><code> </code></th><th><code>VCL(Video Coding Layer) + NAL(Network Abstraction Layer).</code></th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><ol><li>VCL：包括核心压缩引擎和块，宏块和片的语法级别定义，设计目标是尽可能地独立于网络进行高效的编码；</li><li>NAL：负责将VCL产生的比特字符串适配到各种各样的网络和多元环境中，覆盖了所有片级以上的语法级别。</li></ol><p>因为H264最终还是要在网络上进行传输，在传输的时候，网络包的最大传输单元是1500字节，一个H264的帧往往是大于1500字节的，所以需要将一个帧拆成多个包进行传输。这些拆包、组包等工作都在NAL层去处理。</p><h2 id="H264码流结构"><a href="#H264码流结构" class="headerlink" title="H264码流结构"></a>H264码流结构</h2><p>在VCL进行数据传输或存储之前，这些编码的VCL数据，被映射或封装进NAL单元（NALU）。</p><p>H264码流是由一个个的NAL单元组成，其中SPS、PPS、IDR和SLICE是NAL单元某一类型的数据。</p><p><img src="https://img-blog.csdnimg.cn/20200519205514404.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><h3 id="1-H264的NAL单元"><a href="#1-H264的NAL单元" class="headerlink" title="1. H264的NAL单元"></a>1. <strong>H264的NAL单元</strong></h3><p>一个NALU = 一组对应于视频编码的NALU头部信息 + 一个原始字节序列负荷(RBSP,Raw Byte Sequence Payload).</p><p>如图所示，下图中的NALU的头 + RBSP 就相当于一个NALU(Nal Unit),每个单元都按独立的NALU传送。H.264的结构全部都是以NALU为主，理解了NALU，就理解了H.264的结构。<br>一个原始的H.264 NALU 单元常由 [StartCode] [NALU Header] [NALU Payload] 三部分组成，其中 Start Code 用于标示这是一个NALU 单元的开始，必须是”00 00 00 01” 或”00 00 01”</p><p><img src="https://img-blog.csdnimg.cn/20200519205514403.jpeg" alt="img"></p><p><strong><em>\</em>3字节的0x000001只有一种场合下使用，就是一个完整的帧被编为多个slice的时候，包含这些slice的nalu使用3字节起始码。其余场合都是4字节的。**</strong></p><h4 id="NAL-Header"><a href="#NAL-Header" class="headerlink" title="NAL Header"></a>NAL Header</h4><p>NAL单元的头部是由forbidden_bit(1bit)，nal_reference_bit(2bits)（优先级），nal_unit_type(5bits)（类型）三个部分组成的。</p><p>1、F(forbiden):禁止位，占用NAL头的第一个位，当禁止位值为1时表示语法错误；</p><p>2、NRI:参考级别，占用NAL头的第二到第三个位；值越大，该NAL越重要。</p><p>3、Type:Nal单元数据类型，也就是标识该NAL单元的数据类型是哪种，占用NAL头的第四到第8个位；</p><p><img src="https://img-blog.csdnimg.cn/20200519205818705.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>举例来说：</p><p>00 00 00 01 06: SEI信息 </p><p>00 00 00 01 67: 0x67&amp;0x1f = 0x07 :SPS</p><p>00 00 00 01 68: 0x68&amp;0x1f = 0x08 :PPS</p><p>00 00 00 01 65: 0x65&amp;0x1f = 0x05: IDR Slice</p><p>​    在具体介绍NAL数据类型前，有必要知道NAL分为VCL和非VCL的NAL单元。其中SPS、SEI、PPS等非VCL的NAL参数对解码和显示视频都是很有用的。</p><p>​     而另外一个需要了解的概念就是参数集（Parameter sets），参数集是携带解码参数的NAL单元，参数集对于正确解码是非常重要的，在一个有损耗的传输场景中，传输过程中比特列或包可能丢失或损坏，在这种网络环境下，参数集可以通过高质量的服务来发送，比如向前纠错机制或优先级机制。Parameter sets与其之外的句法元素之间的关系如图9所示：</p><p><img src="https://img-blog.csdnimg.cn/20200519205818671.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>每种类型都有代表一种数据类型，比较重要的以下几种做个简单的介绍：</p><p>1、非VCL的NAL数据类型：</p><p>1）、SPS（序列参数集）：SPS对如标识符、帧数以及参考帧数目、解码图像尺寸和帧场模式等解码参数进行标识记录。</p><p>2）、PPS（图像参数集）：PPS对如熵编码类型、有效参考图像的数目和初始化等解码参数进行标志记录。</p><p>3）、SEI(补充增强信息)：这部分参数可作为H264的比特流数据而被传输，每一个SEI信息被封装成一个NAL单元。SEI对于解码器来说可能是有用的，但是对于基本的解码过程来说，并不是必须的。</p><p>@：先标记一下，SPS、PPS内容是编码器给的。（出处的话，慢慢研究）</p><p>2、VCL的NAL数据类型</p><p>1）、 头信息块，包括宏块类型，量化参数，运动矢量。这些信息是最重要的，因为离开他们，被的数据块种的码元都无法使用。该数据分块称为A类数据分块。</p><p>2）、 帧内编码信息数据块，称为B类数据分块。它包含帧内编码宏块类型，帧内编码系数。对应的slice来说，B类数据分块的可用性依赖于A类数据分块。和帧间编码信息数据块不通的是，帧内编码信息能防止进一步的偏差，因此比帧间编码信息更重要。</p><p>3）、 帧间编码信息数据块，称为C类数据分块。它包含帧间编码宏块类型，帧间编码系数。它通常是slice种最大的一部分。帧间编码信息数据块是不重要的一部分。它所包含的信息并不提供编解码器之间的同步。C类数据分块的可用性也依赖于A类数据分块，但于B类数据分块无关。</p><p>以上三种数据块每种分割被单独的存放在一个NAL单元中，因此可以被单独传输。</p><h4 id="RBSP-原始字节序列负荷"><a href="#RBSP-原始字节序列负荷" class="headerlink" title="RBSP**原始字节序列负荷"></a>RBSP**原始字节序列负荷</h4><p><img src="https://img-blog.csdnimg.cn/20200519205818704.jpeg" alt="img"></p><p>序列举例</p><p><img src="https://img-blog.csdnimg.cn/20200519205818689.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p><strong><em>\</em>SODB*****</strong>*与*<strong>**</strong>*RBSP****<br>SODB 数据比特串 -&gt; 是编码后的原始数据.<br>RBSP 原始字节序列载荷 -&gt; 在原始编码数据的后面添加了 结尾比特。一个 bit“1”若干比特“0”，以便字节对齐。<img src="https://img-blog.csdnimg.cn/20200519205818689.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>（1）SODB ，String Of Data Bits 原始数据比特流</p><p>​    因为它是流的形式，所以长度不一定是8倍数，它是由 VLC 层产生的。由于我们计算机是以8倍数去处理数据所以计算机在处理H264时，就需要 RBSP。</p><p>（2）RBSP，SODB + tailing bits （原始字节序列载荷）</p><p>​    由于它是一个压缩流，SODB 不知道是在何处结束，所以算法在SODB最后一位补一个1，没有按字节对齐的则补 0。</p><p>（3）EBSP （扩展字节序列载荷）</p><p>​    在生成压缩流之后，在每一帧的开头加一个起始位，这个起始位一般是 00 00 00 01 或者是 00 00 01。所以在h264码流中规定每有两个连续的00 00，就增加一个0x03。</p><p>补充：EBSP 和 RBSP的区别</p><p>​    A、NALU的主体涉及到三个重要的名词，分别为EBSP、RBSP和SODB。其中EBSP完全等价于NALU主体，而且它们三个的结构关系为：EBSP包含RBSP，RBSP包含SODB。</p><p>​    在h264的文档中，并没有EBSP这一名词出现，但是在h264的官方参考软件JM里，却使用了EBSP。NALU的组成部分为：</p><p>​    NALU = NALU Header + RBSP</p><p>​    其实严格来说，这个等式是不成立的，因为RBSP并不等于NALU刨去NALU Header。严格来说，NALU的组成部分应为：</p><p>​    NALU = NALU Header + EBSP</p><p>​    其中的EBSP为扩展字节序列载荷（Encapsulated Byte Sequence Payload），而RBSP为原始字节序列载荷（Raw Byte Sequence Payload）。</p><p>​     B、那为什么要弄一个EBSP呢？</p><p>​    EBSP相较于RBSP，多了防止竞争的一个字节：0x03。</p><p>​    我们知道，NALU的起始码为0x000001或0x00000001，同时H264规定，当检测到0x000000时，也可以表示当前NALU的结束。那这样就会产生一个问题，就是如果在NALU的内部，出现了0x000001或0x000000时该怎么办？</p><p>​    所以H264就提出了“防止竞争”这样一种机制，当编码器编码完一个NAL时，应该检测NALU内部，是否出现如下左侧的四个序列。当检测到它们存在时，编码器就在最后一个字节前，插入一个新的字节：0x03。</p><p>​    这样一来，当我们拿到EBSP时，就需要检测EBSP内是否有序列：0x000003，如果有，则去掉其中的0x03。这样一来，我们就能得到原始字节序列载荷：RBSP。</p><h4 id="SPS-和-PPS"><a href="#SPS-和-PPS" class="headerlink" title="SPS 和 PPS"></a>SPS 和 PPS</h4><p>SPS即Sequence Paramater Set，又称作序列参数集。SPS中保存了一组编码视频序列(Coded video sequence)的全局参数。存放包括：帧数、参考帧数目、解码图像尺寸、帧场编码模式选择标识等。所谓的编码视频序列即原始视频的一帧一帧的像素数据经过编码之后的结构组成的序列。而每一帧的编码后数据所依赖的参数保存于图像参数集中。一般情况SPS和PPS的NAL Unit通常位于整个码流的起始位置。但在某些特殊情况下，在码流中间也可能出现这两种结构，主要原因可能为：</p><p>解码器需要在码流中间开始解码；<br>编码器在编码的过程中改变了码流的参数（如图像分辨率等）；<br>H.264中另一重要的参数集合为图像参数集Picture Paramater Set(PPS)。和图像相关的参数集，存放包括：熵编码模式选择标识、片组数目、初始量化参数和去方块滤波系数调整标识等。通常情况下，PPS类似于SPS，在H.264的裸码流中单独保存在一个NAL Unit中，只是PPS NAL Unit的nal_unit_type值为8；而在封装格式中，PPS通常与SPS一起，保存在视频文件的文件头中。</p><p>在一组帧之前，首先要收到SPS 和 PPS ，不然的话是无法解码的。这两组数据划分为I帧，是不能丢的。</p><h4 id="H264的NAL单元与片，宏之间的联系"><a href="#H264的NAL单元与片，宏之间的联系" class="headerlink" title="H264的NAL单元与片，宏之间的联系"></a>H264的NAL单元与片，宏之间的联系</h4><p>​     其实到这里可能就比较难理解了，为什么数据NAL单元中有这么多数据类型，这个SLICE又是什么东西，为什么不直接是编码后出来的原始字节序列载荷，所以我觉得在这里再讲述帧所细分的一些片和宏的概念应该是比较合适的，也是能够参照上下文更能理解这些概念的位置，又能给这些困惑做一个合理一点的解释，所以在此做一个描述：</p><p>1帧（一幅图像） = 1~N个片（slice） //也可以说1到多个片为一个片组</p><p>1个片 = 1~N个宏块（Marcroblock）</p><p>1个宏块 = 16X16的YUV数据（原始视频采集数据）</p><p>从数据层次角度来说，一幅原始的图片可以算作广义上的一帧，帧包含片组和片，片组由片来组成，片由宏块来组成，每个宏块可以是4<em>4、8</em>8、16*16像素规模的大小，它们之间的联系如图10所示。每个片都是一个独立的编码单位。</p><p><img src="https://img-blog.csdnimg.cn/20200519205851993.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>​                         图10</p><p>从容纳数据角度来说，NAL单元除了容纳Slice编码的码流外，还可以容纳其他数据，这也就是为什么有SPS、PPS等这些数据出现的原因，并且这些数据在传输H264码流的过程中起到不可或缺的作用，具体作用上面也是有讲到的。</p><p>那么也就可以对下面这些概念做一个大小的排序了：</p><p>​         序列&gt;图像&gt;片&gt;宏&gt;像素（当然还有片组、亚宏块等等这些概念，初步了解就不了解这么深了，后面再慢慢研究）</p><p>同时有几点需要说明一下，这样能便于理解NAL单元：</p><p>（1）、如果不采用 FMO（灵活宏块排序） 机制，则一幅图像只有一个片组；</p><p>（2）、如果不使用多个片，则一个片组只有一个片；</p><p>（3）、如果不采用 DP（数据分割）机制，则一个片就是一个 NALU，一个 NALU 也就是一个片。</p><p>  否则，一个片的组成需要由 三个 NALU 组成，也就是上面说到的A、B、C类数据块。</p><p>这时候在看下面这幅码流数据分层图11就比较能理解整体的码流结构组成了；</p><p><img src="https://img-blog.csdnimg.cn/2020051920585237.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p>​                            图11</p><p>  如我们所见，每个分片也包含着头和数据两部分，分片头中包含着分片类型、分片中的宏块类型、分片帧的数量以及对应的帧的设置和参数等信息，而分片数据中则是宏块，这里就是我们要找的存储像素数据的地方；宏块是视频信息的主要承载者，因为它包含着每一个像素的亮度和色度信息。视频解码最主要的工作则是提供高效的方式从码流中获得宏块中的像素阵列。宏块数据的组成如下图12所示：</p><p><img src="https://img-blog.csdnimg.cn/202005192058526.png" alt="img"></p><p>​                  图12</p><p>  从上图中，可以看到，宏块中包含了宏块类型、预测类型、Coded Block Pattern、Quantization Parameter、像素的亮度和色度数据集等等信息。</p><p>至此，我们对 H.264 的码流数据结构应该有了一个大致的了解。</p><h4 id="Slice-片"><a href="#Slice-片" class="headerlink" title="Slice(**片**)"></a>Slice(*<em>*</em>片*<em>*</em>)</h4><p>如图所示，NALU的主体中包含了Slice(片).</p><table><thead><tr><th></th><th><code>一个片 = Slice Header + Slice Data</code></th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><p>片是H.264提出的新概念，通过编码图片后切分通过高效的方式整合出来的概念。一张图片有一个或者多个片，而片由NALU装载并进行网络传输的。但是NALU不一定是切片，这是充分不必要条件，因为 NALU 还有可能装载着其他用作描述视频的信息.</p><p><strong><em>\</em>那么为什么要设置片呢*****</strong>*?****<br>设置片的目的是为了限制误码的扩散和传输，应使编码片相互间是独立的。某片的预测不能以其他片中的宏块为参考图像，这样某一片中的预测误差才不会传播到其他片中。</p><p>可以看到上图中，每个图像中，若干宏块(Macroblock)被排列成片。一个视频图像可编程一个或更多个片，每片包含整数个宏块 (MB),每片至少包含一个宏块。<br><strong><em>\</em>片有一下五种类型*****</strong>*:****</p><table><thead><tr><th><strong><em>\</em>片**</strong></th><th><strong><em>\</em>意义**</strong></th></tr></thead><tbody><tr><td>I 片</td><td>只包含I宏块</td></tr><tr><td>P 片</td><td>包含P和I宏块</td></tr><tr><td>B 片</td><td>包含B和I宏块</td></tr><tr><td>SP 片</td><td>包含P 和/或 I宏块,用于不同码流之间的切换</td></tr><tr><td>SI 片</td><td>一种特殊类型的编码宏块</td></tr></tbody></table><h4 id="宏块-Macroblock"><a href="#宏块-Macroblock" class="headerlink" title="宏块**(Macroblock)"></a>宏块*<em>*</em>(Macroblock)</h4><p>刚才在片中提到了宏块.<strong><em>\</em>那么什么是宏块呢？**</strong><br>宏块是视频信息的主要承载者。一个编码图像通常划分为多个*<strong>*宏块**</strong>组成.包含着每一个像素的亮度和色度信息。视频解码最主要的工作则是提供高效的方式从码流中获得宏块中像素阵列。</p><table><thead><tr><th><code>1</code></th><th><code>一个宏块 = 一个16*16的亮度像素 + 一个8×8Cb + 一个8×8Cr彩色像素块组成。(YCbCr 是属于 YUV 家族的一员,在YCbCr 中 Y 是指亮度分量，Cb 指蓝色色度分量，而 Cr 指红色色度分量)</code></th></tr></thead><tbody><tr><td></td><td></td></tr></tbody></table><table><thead><tr><th><strong><em>\</em>宏块分类**</strong></th><th><strong><em>\</em>意义**</strong></th></tr></thead><tbody><tr><td>I 宏块</td><td>利用从<strong>当前片</strong>中已解码的像素作为参考进行帧内预测</td></tr><tr><td>P 宏块</td><td>利用前面已编码图像作为参考进行帧内预测，一个帧内编码的宏块可进一步作宏块的分割:即16×16.16×8.8×16.8×8亮度像素块。如果选了8×8的子宏块，则可再分成各种子宏块的分割，其尺寸为8×8，8×4，4×8，4×4</td></tr><tr><td>B 宏块</td><td>利用双向的参考图像(当前和未来的已编码图像帧)进行帧内预测</td></tr></tbody></table><p><img src="https://img-blog.csdnimg.cn/202005192058526.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><p><strong><em>\</em>图*****</strong>*2.1*<strong>**</strong>*句发元素的分层结构****,在 H.264 中，句法元素共被组织成 序列、图像、片、宏块、子宏块五个层次。<br>句法元素的分层结构有助于更有效地节省码流。例如，再一个图像中，经常会在各个片之间有相同的数据，如果每个片都同时携带这些数据，势必会造成码流的浪费。更为有效的做法是将该图像的公共信息抽取出来，形成图像一级的句法元素，而在片级只携带该片自身独有的句法元素。</p><p><img src="https://img-blog.csdnimg.cn/202005192058525.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"><br><strong><em>\</em>图*****</strong>*2.2*<strong>**</strong>*宏块的句法单*<strong>**</strong>*元****</p><p><img src="https://img-blog.csdnimg.cn/2020051920585246.jpeg?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p><table><thead><tr><th><strong><em>\</em>宏块分类**</strong></th><th><strong><em>\</em>意义**</strong></th></tr></thead><tbody><tr><td>mb_type</td><td>确定该 MB 是帧内或帧间(P 或 B)编码模式，确定该 MB 分割的尺寸</td></tr><tr><td>mb_pred</td><td>确定帧内预测模式(帧内宏块)确定表 0 或表 1 参考图 像，和每一宏块分割的差分编码的运动矢量(帧间宏块，除 8×8 宏块分割的帧内 MB)</td></tr><tr><td>sub_mb_pred</td><td>(只对 8×8MB 分割的帧内 MB)确定每一子宏块的子宏 块分割，每一宏块分割的表 0 和/或表 1 的参考图象;每一 宏块子分割的差分编码运动矢量。</td></tr><tr><td>coded_block_pattern</td><td>指出哪个 8×8 块(亮度和彩色)包 编码变换系数</td></tr><tr><td>mb_qp_delta</td><td>量化参数的改变值</td></tr><tr><td>residual</td><td>预测后对应于残差图象取样的编码变换系数</td></tr></tbody></table><h4 id="图像-场和-帧"><a href="#图像-场和-帧" class="headerlink" title="图像**,**场和**帧**"></a>图像*<em>*</em>,*<em>*</em>场和*<em>*</em>帧**</h4><p>图像是个集合概念，顶 场、底场、帧都可以称为图像。对于H.264 协议来说，我们平常所熟悉的那些称呼，例如： I 帧、P 帧、B帧等等，实际上都是我们把图像这个概念具体化和细小化了。我们 在 H.264里提到的“帧”通常就是指不分场的图像；</p><p>视频的一场或一帧可用来产生一个*<strong>*编码图像**</strong>。一帧通常是一个完整的图像。当采集视频信号时，如果采用隔行扫描(奇.偶数行),则扫描下来的一帧图像就被分为了两个部分,这每一部分就被称为 <strong><em>\</em>[*****</strong>*场*<strong>**</strong>*]*<strong>*,根据次序氛围: *</strong>*[*<strong>**</strong>*顶场*<strong>**</strong>*]*<strong>* 和 *</strong>*[*<strong>**</strong>*底场*<strong>**</strong>*]****。</p><table><thead><tr><th><strong><em>\</em>方式**</strong></th><th><strong><em>\</em>作用域**</strong></th></tr></thead><tbody><tr><td>帧编码方式</td><td>活动量较小或者静止的图像宜采用</td></tr><tr><td>场编码方式</td><td>活动量较大的运动图像</td></tr></tbody></table><p><img src="https://img-blog.csdnimg.cn/20200519205852104.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L2d1b3l1bmZlaTEyMw==,size_16,color_FFFFFF,t_70" alt="img"></p>]]></content>
      
      
      <categories>
          
          <category> 音视频基础 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> h264 </tag>
            
            <tag> 转载 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ss</title>
      <link href="2020/11/27/ss/"/>
      <url>2020/11/27/ss/</url>
      
        <content type="html"><![CDATA[]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
